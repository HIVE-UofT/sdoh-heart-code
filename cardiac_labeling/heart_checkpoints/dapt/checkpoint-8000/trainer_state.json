{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.024296108826321,
  "eval_steps": 500,
  "global_step": 8000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.012654223347042075,
      "grad_norm": 1.1209136247634888,
      "learning_rate": 2.45e-05,
      "loss": 3.6952,
      "step": 50
    },
    {
      "epoch": 0.02530844669408415,
      "grad_norm": 0.986005961894989,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 3.1912,
      "step": 100
    },
    {
      "epoch": 0.037962670041126224,
      "grad_norm": 1.1922364234924316,
      "learning_rate": 7.450000000000001e-05,
      "loss": 3.0785,
      "step": 150
    },
    {
      "epoch": 0.0506168933881683,
      "grad_norm": 0.7960503101348877,
      "learning_rate": 9.95e-05,
      "loss": 3.0279,
      "step": 200
    },
    {
      "epoch": 0.06327111673521038,
      "grad_norm": 0.8204024434089661,
      "learning_rate": 9.999383162408304e-05,
      "loss": 2.9903,
      "step": 250
    },
    {
      "epoch": 0.07592534008225245,
      "grad_norm": 0.9740110039710999,
      "learning_rate": 9.997482198200576e-05,
      "loss": 2.9772,
      "step": 300
    },
    {
      "epoch": 0.08857956342929453,
      "grad_norm": 0.8941901326179504,
      "learning_rate": 9.994297338836592e-05,
      "loss": 2.9983,
      "step": 350
    },
    {
      "epoch": 0.1012337867763366,
      "grad_norm": 0.8091728091239929,
      "learning_rate": 9.989829402533477e-05,
      "loss": 2.9671,
      "step": 400
    },
    {
      "epoch": 0.11388801012337868,
      "grad_norm": 0.762223482131958,
      "learning_rate": 9.984079537141598e-05,
      "loss": 2.9256,
      "step": 450
    },
    {
      "epoch": 0.12654223347042076,
      "grad_norm": 0.8506777882575989,
      "learning_rate": 9.977049219849672e-05,
      "loss": 2.9439,
      "step": 500
    },
    {
      "epoch": 0.13919645681746282,
      "grad_norm": 0.9150068163871765,
      "learning_rate": 9.968740256805269e-05,
      "loss": 2.9468,
      "step": 550
    },
    {
      "epoch": 0.1518506801645049,
      "grad_norm": 0.6906594038009644,
      "learning_rate": 9.95915478265079e-05,
      "loss": 2.9223,
      "step": 600
    },
    {
      "epoch": 0.16450490351154698,
      "grad_norm": 0.8396060466766357,
      "learning_rate": 9.948295259975066e-05,
      "loss": 2.9565,
      "step": 650
    },
    {
      "epoch": 0.17715912685858906,
      "grad_norm": 0.9706467986106873,
      "learning_rate": 9.936164478680697e-05,
      "loss": 2.9464,
      "step": 700
    },
    {
      "epoch": 0.18981335020563114,
      "grad_norm": 0.7664065957069397,
      "learning_rate": 9.92276555526729e-05,
      "loss": 2.9222,
      "step": 750
    },
    {
      "epoch": 0.2024675735526732,
      "grad_norm": 0.8561677932739258,
      "learning_rate": 9.908101932030829e-05,
      "loss": 2.9121,
      "step": 800
    },
    {
      "epoch": 0.21512179689971528,
      "grad_norm": 0.7816307544708252,
      "learning_rate": 9.892177376179296e-05,
      "loss": 2.9056,
      "step": 850
    },
    {
      "epoch": 0.22777602024675736,
      "grad_norm": 0.6966648697853088,
      "learning_rate": 9.874995978864861e-05,
      "loss": 2.9387,
      "step": 900
    },
    {
      "epoch": 0.24043024359379944,
      "grad_norm": 0.7397676110267639,
      "learning_rate": 9.856562154132817e-05,
      "loss": 2.9125,
      "step": 950
    },
    {
      "epoch": 0.2530844669408415,
      "grad_norm": 0.7057637572288513,
      "learning_rate": 9.836880637787588e-05,
      "loss": 2.8959,
      "step": 1000
    },
    {
      "epoch": 0.2657386902878836,
      "grad_norm": 0.7692059278488159,
      "learning_rate": 9.815956486176048e-05,
      "loss": 2.9316,
      "step": 1050
    },
    {
      "epoch": 0.27839291363492563,
      "grad_norm": 0.7832862734794617,
      "learning_rate": 9.793795074888512e-05,
      "loss": 2.8675,
      "step": 1100
    },
    {
      "epoch": 0.2910471369819677,
      "grad_norm": 0.7507484555244446,
      "learning_rate": 9.770402097377698e-05,
      "loss": 2.8418,
      "step": 1150
    },
    {
      "epoch": 0.3037013603290098,
      "grad_norm": 0.8880273103713989,
      "learning_rate": 9.745783563496034e-05,
      "loss": 2.8743,
      "step": 1200
    },
    {
      "epoch": 0.3163555836760519,
      "grad_norm": 0.7424572110176086,
      "learning_rate": 9.719945797951675e-05,
      "loss": 2.8759,
      "step": 1250
    },
    {
      "epoch": 0.32900980702309396,
      "grad_norm": 1.0142822265625,
      "learning_rate": 9.692895438683627e-05,
      "loss": 2.8879,
      "step": 1300
    },
    {
      "epoch": 0.34166403037013604,
      "grad_norm": 0.9829534888267517,
      "learning_rate": 9.664639435156414e-05,
      "loss": 2.9039,
      "step": 1350
    },
    {
      "epoch": 0.3543182537171781,
      "grad_norm": 0.862928569316864,
      "learning_rate": 9.635185046574695e-05,
      "loss": 2.906,
      "step": 1400
    },
    {
      "epoch": 0.3669724770642202,
      "grad_norm": 0.684027910232544,
      "learning_rate": 9.604539840018321e-05,
      "loss": 2.8696,
      "step": 1450
    },
    {
      "epoch": 0.3796267004112623,
      "grad_norm": 0.7741448879241943,
      "learning_rate": 9.572711688498276e-05,
      "loss": 2.8733,
      "step": 1500
    },
    {
      "epoch": 0.3922809237583043,
      "grad_norm": 0.8009865283966064,
      "learning_rate": 9.53970876893405e-05,
      "loss": 2.8939,
      "step": 1550
    },
    {
      "epoch": 0.4049351471053464,
      "grad_norm": 0.7556995153427124,
      "learning_rate": 9.505539560052905e-05,
      "loss": 2.909,
      "step": 1600
    },
    {
      "epoch": 0.4175893704523885,
      "grad_norm": 0.7408649921417236,
      "learning_rate": 9.470212840211632e-05,
      "loss": 2.9047,
      "step": 1650
    },
    {
      "epoch": 0.43024359379943056,
      "grad_norm": 0.8015015721321106,
      "learning_rate": 9.433737685141305e-05,
      "loss": 2.9206,
      "step": 1700
    },
    {
      "epoch": 0.44289781714647264,
      "grad_norm": 0.832034170627594,
      "learning_rate": 9.396123465615658e-05,
      "loss": 2.91,
      "step": 1750
    },
    {
      "epoch": 0.4555520404935147,
      "grad_norm": 0.6817078590393066,
      "learning_rate": 9.357379845043654e-05,
      "loss": 2.893,
      "step": 1800
    },
    {
      "epoch": 0.4682062638405568,
      "grad_norm": 0.6884402632713318,
      "learning_rate": 9.317516776986866e-05,
      "loss": 2.8836,
      "step": 1850
    },
    {
      "epoch": 0.4808604871875989,
      "grad_norm": 0.6444289684295654,
      "learning_rate": 9.276544502602337e-05,
      "loss": 2.8569,
      "step": 1900
    },
    {
      "epoch": 0.4935147105346409,
      "grad_norm": 0.8229833841323853,
      "learning_rate": 9.234473548011527e-05,
      "loss": 2.9008,
      "step": 1950
    },
    {
      "epoch": 0.506168933881683,
      "grad_norm": 0.8413760662078857,
      "learning_rate": 9.191314721596072e-05,
      "loss": 2.8887,
      "step": 2000
    },
    {
      "epoch": 0.5188231572287251,
      "grad_norm": 0.7692490220069885,
      "learning_rate": 9.147079111221018e-05,
      "loss": 2.877,
      "step": 2050
    },
    {
      "epoch": 0.5314773805757672,
      "grad_norm": 0.5834383368492126,
      "learning_rate": 9.101778081386257e-05,
      "loss": 2.8686,
      "step": 2100
    },
    {
      "epoch": 0.5441316039228092,
      "grad_norm": 0.8569998145103455,
      "learning_rate": 9.055423270306889e-05,
      "loss": 2.901,
      "step": 2150
    },
    {
      "epoch": 0.5567858272698513,
      "grad_norm": 0.7364405393600464,
      "learning_rate": 9.008026586923264e-05,
      "loss": 2.8762,
      "step": 2200
    },
    {
      "epoch": 0.5694400506168934,
      "grad_norm": 0.7533401250839233,
      "learning_rate": 8.959600207841483e-05,
      "loss": 2.8682,
      "step": 2250
    },
    {
      "epoch": 0.5820942739639354,
      "grad_norm": 0.7989473342895508,
      "learning_rate": 8.910156574205121e-05,
      "loss": 2.8661,
      "step": 2300
    },
    {
      "epoch": 0.5947484973109776,
      "grad_norm": 0.6763147115707397,
      "learning_rate": 8.859708388498996e-05,
      "loss": 2.8468,
      "step": 2350
    },
    {
      "epoch": 0.6074027206580196,
      "grad_norm": 0.7763379216194153,
      "learning_rate": 8.80826861128578e-05,
      "loss": 2.8309,
      "step": 2400
    },
    {
      "epoch": 0.6200569440050617,
      "grad_norm": 0.8251469731330872,
      "learning_rate": 8.755850457876347e-05,
      "loss": 2.8927,
      "step": 2450
    },
    {
      "epoch": 0.6327111673521038,
      "grad_norm": 0.6280380487442017,
      "learning_rate": 8.702467394934624e-05,
      "loss": 2.8454,
      "step": 2500
    },
    {
      "epoch": 0.6453653906991459,
      "grad_norm": 0.6702473759651184,
      "learning_rate": 8.648133137017906e-05,
      "loss": 2.879,
      "step": 2550
    },
    {
      "epoch": 0.6580196140461879,
      "grad_norm": 0.7802298665046692,
      "learning_rate": 8.592861643053469e-05,
      "loss": 2.8943,
      "step": 2600
    },
    {
      "epoch": 0.6706738373932299,
      "grad_norm": 0.6800094842910767,
      "learning_rate": 8.536667112752399e-05,
      "loss": 2.8603,
      "step": 2650
    },
    {
      "epoch": 0.6833280607402721,
      "grad_norm": 0.7358947396278381,
      "learning_rate": 8.479563982961571e-05,
      "loss": 2.8854,
      "step": 2700
    },
    {
      "epoch": 0.6959822840873141,
      "grad_norm": 0.5677722096443176,
      "learning_rate": 8.421566923954707e-05,
      "loss": 2.8628,
      "step": 2750
    },
    {
      "epoch": 0.7086365074343562,
      "grad_norm": 0.7850372195243835,
      "learning_rate": 8.362690835663445e-05,
      "loss": 2.9034,
      "step": 2800
    },
    {
      "epoch": 0.7212907307813983,
      "grad_norm": 0.6631876230239868,
      "learning_rate": 8.302950843849436e-05,
      "loss": 2.855,
      "step": 2850
    },
    {
      "epoch": 0.7339449541284404,
      "grad_norm": 0.7861546277999878,
      "learning_rate": 8.242362296218397e-05,
      "loss": 2.8788,
      "step": 2900
    },
    {
      "epoch": 0.7465991774754824,
      "grad_norm": 0.6736059784889221,
      "learning_rate": 8.180940758477175e-05,
      "loss": 2.8508,
      "step": 2950
    },
    {
      "epoch": 0.7592534008225246,
      "grad_norm": 0.6897044777870178,
      "learning_rate": 8.118702010334758e-05,
      "loss": 2.8604,
      "step": 3000
    },
    {
      "epoch": 0.7719076241695666,
      "grad_norm": 0.7634857296943665,
      "learning_rate": 8.05566204144836e-05,
      "loss": 2.8729,
      "step": 3050
    },
    {
      "epoch": 0.7845618475166086,
      "grad_norm": 0.795070469379425,
      "learning_rate": 7.991837047315532e-05,
      "loss": 2.8845,
      "step": 3100
    },
    {
      "epoch": 0.7972160708636508,
      "grad_norm": 0.7932234406471252,
      "learning_rate": 7.927243425113407e-05,
      "loss": 2.873,
      "step": 3150
    },
    {
      "epoch": 0.8098702942106928,
      "grad_norm": 0.553981602191925,
      "learning_rate": 7.861897769486121e-05,
      "loss": 2.829,
      "step": 3200
    },
    {
      "epoch": 0.8225245175577349,
      "grad_norm": 0.832590639591217,
      "learning_rate": 7.795816868281521e-05,
      "loss": 2.8406,
      "step": 3250
    },
    {
      "epoch": 0.835178740904777,
      "grad_norm": 0.8086447715759277,
      "learning_rate": 7.729017698238212e-05,
      "loss": 2.8673,
      "step": 3300
    },
    {
      "epoch": 0.8478329642518191,
      "grad_norm": 0.7937609553337097,
      "learning_rate": 7.661517420624101e-05,
      "loss": 2.8487,
      "step": 3350
    },
    {
      "epoch": 0.8604871875988611,
      "grad_norm": 0.6960878968238831,
      "learning_rate": 7.59333337682752e-05,
      "loss": 2.8637,
      "step": 3400
    },
    {
      "epoch": 0.8731414109459031,
      "grad_norm": 0.893937349319458,
      "learning_rate": 7.524483083902076e-05,
      "loss": 2.8589,
      "step": 3450
    },
    {
      "epoch": 0.8857956342929453,
      "grad_norm": 0.6948588490486145,
      "learning_rate": 7.454984230066371e-05,
      "loss": 2.8523,
      "step": 3500
    },
    {
      "epoch": 0.8984498576399873,
      "grad_norm": 0.6877617239952087,
      "learning_rate": 7.384854670159757e-05,
      "loss": 2.8776,
      "step": 3550
    },
    {
      "epoch": 0.9111040809870294,
      "grad_norm": 0.8207176923751831,
      "learning_rate": 7.31411242105527e-05,
      "loss": 2.8849,
      "step": 3600
    },
    {
      "epoch": 0.9237583043340715,
      "grad_norm": 0.710628092288971,
      "learning_rate": 7.242775657030954e-05,
      "loss": 2.8618,
      "step": 3650
    },
    {
      "epoch": 0.9364125276811136,
      "grad_norm": 0.48997363448143005,
      "learning_rate": 7.17086270510072e-05,
      "loss": 2.8537,
      "step": 3700
    },
    {
      "epoch": 0.9490667510281556,
      "grad_norm": 0.6646195650100708,
      "learning_rate": 7.098392040306001e-05,
      "loss": 2.8508,
      "step": 3750
    },
    {
      "epoch": 0.9617209743751978,
      "grad_norm": 0.7768404483795166,
      "learning_rate": 7.025382280969346e-05,
      "loss": 2.803,
      "step": 3800
    },
    {
      "epoch": 0.9743751977222398,
      "grad_norm": 0.7650391459465027,
      "learning_rate": 6.95185218391122e-05,
      "loss": 2.8555,
      "step": 3850
    },
    {
      "epoch": 0.9870294210692818,
      "grad_norm": 0.8182047605514526,
      "learning_rate": 6.877820639631222e-05,
      "loss": 2.8246,
      "step": 3900
    },
    {
      "epoch": 0.999683644416324,
      "grad_norm": 0.5656609535217285,
      "learning_rate": 6.80330666745495e-05,
      "loss": 2.8334,
      "step": 3950
    },
    {
      "epoch": 1.0121480544131605,
      "grad_norm": 0.8596081137657166,
      "learning_rate": 6.728329410647782e-05,
      "loss": 2.7281,
      "step": 4000
    },
    {
      "epoch": 1.0248022777602024,
      "grad_norm": 0.8331876993179321,
      "learning_rate": 6.652908131496817e-05,
      "loss": 2.7438,
      "step": 4050
    },
    {
      "epoch": 1.0374565011072445,
      "grad_norm": 0.9927542209625244,
      "learning_rate": 6.577062206362215e-05,
      "loss": 2.7478,
      "step": 4100
    },
    {
      "epoch": 1.0501107244542867,
      "grad_norm": 0.7503067851066589,
      "learning_rate": 6.500811120699256e-05,
      "loss": 2.776,
      "step": 4150
    },
    {
      "epoch": 1.0627649478013288,
      "grad_norm": 0.8594722151756287,
      "learning_rate": 6.424174464052372e-05,
      "loss": 2.776,
      "step": 4200
    },
    {
      "epoch": 1.0754191711483707,
      "grad_norm": 0.7482352256774902,
      "learning_rate": 6.34717192502241e-05,
      "loss": 2.7383,
      "step": 4250
    },
    {
      "epoch": 1.0880733944954128,
      "grad_norm": 0.9169026613235474,
      "learning_rate": 6.26982328620848e-05,
      "loss": 2.7739,
      "step": 4300
    },
    {
      "epoch": 1.100727617842455,
      "grad_norm": 0.7162443399429321,
      "learning_rate": 6.192148419125635e-05,
      "loss": 2.7371,
      "step": 4350
    },
    {
      "epoch": 1.113381841189497,
      "grad_norm": 0.707607090473175,
      "learning_rate": 6.114167279099706e-05,
      "loss": 2.7511,
      "step": 4400
    },
    {
      "epoch": 1.126036064536539,
      "grad_norm": 0.7723420262336731,
      "learning_rate": 6.0358999001406156e-05,
      "loss": 2.7575,
      "step": 4450
    },
    {
      "epoch": 1.1386902878835812,
      "grad_norm": 0.7098573446273804,
      "learning_rate": 5.957366389795478e-05,
      "loss": 2.749,
      "step": 4500
    },
    {
      "epoch": 1.1513445112306233,
      "grad_norm": 0.9452178478240967,
      "learning_rate": 5.878586923982793e-05,
      "loss": 2.7611,
      "step": 4550
    },
    {
      "epoch": 1.1639987345776652,
      "grad_norm": 0.7143250107765198,
      "learning_rate": 5.799581741809086e-05,
      "loss": 2.7552,
      "step": 4600
    },
    {
      "epoch": 1.1766529579247074,
      "grad_norm": 0.7778602838516235,
      "learning_rate": 5.7203711403693097e-05,
      "loss": 2.7804,
      "step": 4650
    },
    {
      "epoch": 1.1893071812717495,
      "grad_norm": 0.999950110912323,
      "learning_rate": 5.640975469532358e-05,
      "loss": 2.7526,
      "step": 4700
    },
    {
      "epoch": 1.2019614046187916,
      "grad_norm": 0.8183728456497192,
      "learning_rate": 5.561415126713001e-05,
      "loss": 2.7321,
      "step": 4750
    },
    {
      "epoch": 1.2146156279658336,
      "grad_norm": 0.7287235856056213,
      "learning_rate": 5.481710551631626e-05,
      "loss": 2.7443,
      "step": 4800
    },
    {
      "epoch": 1.2272698513128757,
      "grad_norm": 0.8611195683479309,
      "learning_rate": 5.4018822210631024e-05,
      "loss": 2.7363,
      "step": 4850
    },
    {
      "epoch": 1.2399240746599178,
      "grad_norm": 1.0261605978012085,
      "learning_rate": 5.321950643576123e-05,
      "loss": 2.7474,
      "step": 4900
    },
    {
      "epoch": 1.2525782980069597,
      "grad_norm": 0.9046388268470764,
      "learning_rate": 5.241936354264377e-05,
      "loss": 2.7566,
      "step": 4950
    },
    {
      "epoch": 1.2652325213540019,
      "grad_norm": 0.7704071402549744,
      "learning_rate": 5.161859909470924e-05,
      "loss": 2.7425,
      "step": 5000
    },
    {
      "epoch": 1.277886744701044,
      "grad_norm": 0.8356084227561951,
      "learning_rate": 5.0817418815070804e-05,
      "loss": 2.7274,
      "step": 5050
    },
    {
      "epoch": 1.2905409680480862,
      "grad_norm": 0.8617225885391235,
      "learning_rate": 5.001602853367236e-05,
      "loss": 2.7479,
      "step": 5100
    },
    {
      "epoch": 1.303195191395128,
      "grad_norm": 0.7676846385002136,
      "learning_rate": 4.921463413440898e-05,
      "loss": 2.73,
      "step": 5150
    },
    {
      "epoch": 1.3158494147421702,
      "grad_norm": 0.8255720138549805,
      "learning_rate": 4.841344150223364e-05,
      "loss": 2.7358,
      "step": 5200
    },
    {
      "epoch": 1.3285036380892123,
      "grad_norm": 0.9754754304885864,
      "learning_rate": 4.761265647026368e-05,
      "loss": 2.7344,
      "step": 5250
    },
    {
      "epoch": 1.3411578614362543,
      "grad_norm": 0.8811067342758179,
      "learning_rate": 4.6812484766900536e-05,
      "loss": 2.768,
      "step": 5300
    },
    {
      "epoch": 1.3538120847832964,
      "grad_norm": 0.9092094302177429,
      "learning_rate": 4.601313196297633e-05,
      "loss": 2.7428,
      "step": 5350
    },
    {
      "epoch": 1.3664663081303385,
      "grad_norm": 1.073135495185852,
      "learning_rate": 4.521480341894108e-05,
      "loss": 2.7838,
      "step": 5400
    },
    {
      "epoch": 1.3791205314773807,
      "grad_norm": 0.912555456161499,
      "learning_rate": 4.441770423210381e-05,
      "loss": 2.7582,
      "step": 5450
    },
    {
      "epoch": 1.3917747548244226,
      "grad_norm": 0.9731561541557312,
      "learning_rate": 4.362203918394138e-05,
      "loss": 2.7568,
      "step": 5500
    },
    {
      "epoch": 1.4044289781714647,
      "grad_norm": 0.9501434564590454,
      "learning_rate": 4.2828012687488325e-05,
      "loss": 2.7344,
      "step": 5550
    },
    {
      "epoch": 1.4170832015185069,
      "grad_norm": 0.8688806891441345,
      "learning_rate": 4.203582873482155e-05,
      "loss": 2.7391,
      "step": 5600
    },
    {
      "epoch": 1.4297374248655488,
      "grad_norm": 0.912173330783844,
      "learning_rate": 4.124569084465293e-05,
      "loss": 2.7796,
      "step": 5650
    },
    {
      "epoch": 1.442391648212591,
      "grad_norm": 0.7915304899215698,
      "learning_rate": 4.045780201004371e-05,
      "loss": 2.7674,
      "step": 5700
    },
    {
      "epoch": 1.455045871559633,
      "grad_norm": 0.9985153675079346,
      "learning_rate": 3.967236464625391e-05,
      "loss": 2.7471,
      "step": 5750
    },
    {
      "epoch": 1.4677000949066752,
      "grad_norm": 0.8604495525360107,
      "learning_rate": 3.888958053874002e-05,
      "loss": 2.7435,
      "step": 5800
    },
    {
      "epoch": 1.480354318253717,
      "grad_norm": 0.972727358341217,
      "learning_rate": 3.810965079131479e-05,
      "loss": 2.7381,
      "step": 5850
    },
    {
      "epoch": 1.4930085416007592,
      "grad_norm": 0.8187834024429321,
      "learning_rate": 3.733277577448183e-05,
      "loss": 2.7779,
      "step": 5900
    },
    {
      "epoch": 1.5056627649478014,
      "grad_norm": 0.7819388508796692,
      "learning_rate": 3.655915507395884e-05,
      "loss": 2.7248,
      "step": 5950
    },
    {
      "epoch": 1.5183169882948433,
      "grad_norm": 0.8643521070480347,
      "learning_rate": 3.5788987439402295e-05,
      "loss": 2.744,
      "step": 6000
    },
    {
      "epoch": 1.5309712116418854,
      "grad_norm": 0.893679141998291,
      "learning_rate": 3.502247073334705e-05,
      "loss": 2.7371,
      "step": 6050
    },
    {
      "epoch": 1.5436254349889276,
      "grad_norm": 1.0053715705871582,
      "learning_rate": 3.4259801880373685e-05,
      "loss": 2.7395,
      "step": 6100
    },
    {
      "epoch": 1.5562796583359697,
      "grad_norm": 0.9497287273406982,
      "learning_rate": 3.3501176816517086e-05,
      "loss": 2.7502,
      "step": 6150
    },
    {
      "epoch": 1.5689338816830118,
      "grad_norm": 1.0172470808029175,
      "learning_rate": 3.274679043892872e-05,
      "loss": 2.7361,
      "step": 6200
    },
    {
      "epoch": 1.5815881050300538,
      "grad_norm": 0.9806029200553894,
      "learning_rate": 3.1996836555805913e-05,
      "loss": 2.749,
      "step": 6250
    },
    {
      "epoch": 1.594242328377096,
      "grad_norm": 0.8143768906593323,
      "learning_rate": 3.125150783660098e-05,
      "loss": 2.7441,
      "step": 6300
    },
    {
      "epoch": 1.6068965517241378,
      "grad_norm": 0.9532932639122009,
      "learning_rate": 3.051099576252273e-05,
      "loss": 2.7434,
      "step": 6350
    },
    {
      "epoch": 1.61955077507118,
      "grad_norm": 0.8933330774307251,
      "learning_rate": 2.9775490577343423e-05,
      "loss": 2.7157,
      "step": 6400
    },
    {
      "epoch": 1.632204998418222,
      "grad_norm": 0.8493188619613647,
      "learning_rate": 2.904518123852344e-05,
      "loss": 2.7373,
      "step": 6450
    },
    {
      "epoch": 1.6448592217652642,
      "grad_norm": 1.02920401096344,
      "learning_rate": 2.832025536866665e-05,
      "loss": 2.7416,
      "step": 6500
    },
    {
      "epoch": 1.6575134451123064,
      "grad_norm": 0.852422297000885,
      "learning_rate": 2.7600899207318465e-05,
      "loss": 2.7411,
      "step": 6550
    },
    {
      "epoch": 1.6701676684593483,
      "grad_norm": 0.980833888053894,
      "learning_rate": 2.6887297563119485e-05,
      "loss": 2.7163,
      "step": 6600
    },
    {
      "epoch": 1.6828218918063904,
      "grad_norm": 0.9805009961128235,
      "learning_rate": 2.6179633766326404e-05,
      "loss": 2.7635,
      "step": 6650
    },
    {
      "epoch": 1.6954761151534323,
      "grad_norm": 0.916579008102417,
      "learning_rate": 2.5478089621713092e-05,
      "loss": 2.7596,
      "step": 6700
    },
    {
      "epoch": 1.7081303385004745,
      "grad_norm": 0.9336401224136353,
      "learning_rate": 2.47828453618634e-05,
      "loss": 2.7249,
      "step": 6750
    },
    {
      "epoch": 1.7207845618475166,
      "grad_norm": 0.7634913921356201,
      "learning_rate": 2.4094079600867808e-05,
      "loss": 2.7524,
      "step": 6800
    },
    {
      "epoch": 1.7334387851945587,
      "grad_norm": 1.043974757194519,
      "learning_rate": 2.3411969288436054e-05,
      "loss": 2.7372,
      "step": 6850
    },
    {
      "epoch": 1.7460930085416009,
      "grad_norm": 1.0158292055130005,
      "learning_rate": 2.2736689664437217e-05,
      "loss": 2.7339,
      "step": 6900
    },
    {
      "epoch": 1.7587472318886428,
      "grad_norm": 1.0318511724472046,
      "learning_rate": 2.206841421387914e-05,
      "loss": 2.7354,
      "step": 6950
    },
    {
      "epoch": 1.771401455235685,
      "grad_norm": 0.9421192407608032,
      "learning_rate": 2.1407314622338663e-05,
      "loss": 2.735,
      "step": 7000
    },
    {
      "epoch": 1.7840556785827268,
      "grad_norm": 1.0921144485473633,
      "learning_rate": 2.075356073185422e-05,
      "loss": 2.7765,
      "step": 7050
    },
    {
      "epoch": 1.796709901929769,
      "grad_norm": 0.8282895684242249,
      "learning_rate": 2.01073204972919e-05,
      "loss": 2.7641,
      "step": 7100
    },
    {
      "epoch": 1.8093641252768111,
      "grad_norm": 0.9453224539756775,
      "learning_rate": 1.94687599431964e-05,
      "loss": 2.7278,
      "step": 7150
    },
    {
      "epoch": 1.8220183486238533,
      "grad_norm": 0.8974723219871521,
      "learning_rate": 1.883804312113813e-05,
      "loss": 2.7403,
      "step": 7200
    },
    {
      "epoch": 1.8346725719708954,
      "grad_norm": 1.0109542608261108,
      "learning_rate": 1.8215332067566764e-05,
      "loss": 2.7553,
      "step": 7250
    },
    {
      "epoch": 1.8473267953179373,
      "grad_norm": 1.0055581331253052,
      "learning_rate": 1.760078676218283e-05,
      "loss": 2.745,
      "step": 7300
    },
    {
      "epoch": 1.8599810186649794,
      "grad_norm": 0.8076223731040955,
      "learning_rate": 1.6994565086837515e-05,
      "loss": 2.7183,
      "step": 7350
    },
    {
      "epoch": 1.8726352420120214,
      "grad_norm": 0.9189767837524414,
      "learning_rate": 1.6396822784971628e-05,
      "loss": 2.7401,
      "step": 7400
    },
    {
      "epoch": 1.8852894653590635,
      "grad_norm": 0.7865903377532959,
      "learning_rate": 1.5807713421603555e-05,
      "loss": 2.7414,
      "step": 7450
    },
    {
      "epoch": 1.8979436887061056,
      "grad_norm": 1.1994922161102295,
      "learning_rate": 1.5227388343877402e-05,
      "loss": 2.7076,
      "step": 7500
    },
    {
      "epoch": 1.9105979120531478,
      "grad_norm": 0.9008607864379883,
      "learning_rate": 1.4655996642180425e-05,
      "loss": 2.7018,
      "step": 7550
    },
    {
      "epoch": 1.92325213540019,
      "grad_norm": 0.9537704586982727,
      "learning_rate": 1.4093685111840566e-05,
      "loss": 2.7229,
      "step": 7600
    },
    {
      "epoch": 1.935906358747232,
      "grad_norm": 0.8947159647941589,
      "learning_rate": 1.3540598215413458e-05,
      "loss": 2.744,
      "step": 7650
    },
    {
      "epoch": 1.948560582094274,
      "grad_norm": 0.8463431596755981,
      "learning_rate": 1.299687804556879e-05,
      "loss": 2.7692,
      "step": 7700
    },
    {
      "epoch": 1.9612148054413159,
      "grad_norm": 0.9698774814605713,
      "learning_rate": 1.2462664288585557e-05,
      "loss": 2.7174,
      "step": 7750
    },
    {
      "epoch": 1.973869028788358,
      "grad_norm": 0.9959140419960022,
      "learning_rate": 1.1938094188465343e-05,
      "loss": 2.7357,
      "step": 7800
    },
    {
      "epoch": 1.9865232521354002,
      "grad_norm": 0.9943249225616455,
      "learning_rate": 1.1423302511673372e-05,
      "loss": 2.7445,
      "step": 7850
    },
    {
      "epoch": 1.9991774754824423,
      "grad_norm": 0.9517405033111572,
      "learning_rate": 1.0918421512515719e-05,
      "loss": 2.7253,
      "step": 7900
    },
    {
      "epoch": 2.011641885479279,
      "grad_norm": 1.0122919082641602,
      "learning_rate": 1.0423580899162133e-05,
      "loss": 2.6451,
      "step": 7950
    },
    {
      "epoch": 2.024296108826321,
      "grad_norm": 0.8149110078811646,
      "learning_rate": 9.93890780032291e-06,
      "loss": 2.6656,
      "step": 8000
    }
  ],
  "logging_steps": 50,
  "max_steps": 10000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.0705467467546214e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
