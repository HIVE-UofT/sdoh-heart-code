{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.2652325213540019,
  "eval_steps": 500,
  "global_step": 5000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.012654223347042075,
      "grad_norm": 1.1209136247634888,
      "learning_rate": 2.45e-05,
      "loss": 3.6952,
      "step": 50
    },
    {
      "epoch": 0.02530844669408415,
      "grad_norm": 0.986005961894989,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 3.1912,
      "step": 100
    },
    {
      "epoch": 0.037962670041126224,
      "grad_norm": 1.1922364234924316,
      "learning_rate": 7.450000000000001e-05,
      "loss": 3.0785,
      "step": 150
    },
    {
      "epoch": 0.0506168933881683,
      "grad_norm": 0.7960503101348877,
      "learning_rate": 9.95e-05,
      "loss": 3.0279,
      "step": 200
    },
    {
      "epoch": 0.06327111673521038,
      "grad_norm": 0.8204024434089661,
      "learning_rate": 9.999383162408304e-05,
      "loss": 2.9903,
      "step": 250
    },
    {
      "epoch": 0.07592534008225245,
      "grad_norm": 0.9740110039710999,
      "learning_rate": 9.997482198200576e-05,
      "loss": 2.9772,
      "step": 300
    },
    {
      "epoch": 0.08857956342929453,
      "grad_norm": 0.8941901326179504,
      "learning_rate": 9.994297338836592e-05,
      "loss": 2.9983,
      "step": 350
    },
    {
      "epoch": 0.1012337867763366,
      "grad_norm": 0.8091728091239929,
      "learning_rate": 9.989829402533477e-05,
      "loss": 2.9671,
      "step": 400
    },
    {
      "epoch": 0.11388801012337868,
      "grad_norm": 0.762223482131958,
      "learning_rate": 9.984079537141598e-05,
      "loss": 2.9256,
      "step": 450
    },
    {
      "epoch": 0.12654223347042076,
      "grad_norm": 0.8506777882575989,
      "learning_rate": 9.977049219849672e-05,
      "loss": 2.9439,
      "step": 500
    },
    {
      "epoch": 0.13919645681746282,
      "grad_norm": 0.9150068163871765,
      "learning_rate": 9.968740256805269e-05,
      "loss": 2.9468,
      "step": 550
    },
    {
      "epoch": 0.1518506801645049,
      "grad_norm": 0.6906594038009644,
      "learning_rate": 9.95915478265079e-05,
      "loss": 2.9223,
      "step": 600
    },
    {
      "epoch": 0.16450490351154698,
      "grad_norm": 0.8396060466766357,
      "learning_rate": 9.948295259975066e-05,
      "loss": 2.9565,
      "step": 650
    },
    {
      "epoch": 0.17715912685858906,
      "grad_norm": 0.9706467986106873,
      "learning_rate": 9.936164478680697e-05,
      "loss": 2.9464,
      "step": 700
    },
    {
      "epoch": 0.18981335020563114,
      "grad_norm": 0.7664065957069397,
      "learning_rate": 9.92276555526729e-05,
      "loss": 2.9222,
      "step": 750
    },
    {
      "epoch": 0.2024675735526732,
      "grad_norm": 0.8561677932739258,
      "learning_rate": 9.908101932030829e-05,
      "loss": 2.9121,
      "step": 800
    },
    {
      "epoch": 0.21512179689971528,
      "grad_norm": 0.7816307544708252,
      "learning_rate": 9.892177376179296e-05,
      "loss": 2.9056,
      "step": 850
    },
    {
      "epoch": 0.22777602024675736,
      "grad_norm": 0.6966648697853088,
      "learning_rate": 9.874995978864861e-05,
      "loss": 2.9387,
      "step": 900
    },
    {
      "epoch": 0.24043024359379944,
      "grad_norm": 0.7397676110267639,
      "learning_rate": 9.856562154132817e-05,
      "loss": 2.9125,
      "step": 950
    },
    {
      "epoch": 0.2530844669408415,
      "grad_norm": 0.7057637572288513,
      "learning_rate": 9.836880637787588e-05,
      "loss": 2.8959,
      "step": 1000
    },
    {
      "epoch": 0.2657386902878836,
      "grad_norm": 0.7692059278488159,
      "learning_rate": 9.815956486176048e-05,
      "loss": 2.9316,
      "step": 1050
    },
    {
      "epoch": 0.27839291363492563,
      "grad_norm": 0.7832862734794617,
      "learning_rate": 9.793795074888512e-05,
      "loss": 2.8675,
      "step": 1100
    },
    {
      "epoch": 0.2910471369819677,
      "grad_norm": 0.7507484555244446,
      "learning_rate": 9.770402097377698e-05,
      "loss": 2.8418,
      "step": 1150
    },
    {
      "epoch": 0.3037013603290098,
      "grad_norm": 0.8880273103713989,
      "learning_rate": 9.745783563496034e-05,
      "loss": 2.8743,
      "step": 1200
    },
    {
      "epoch": 0.3163555836760519,
      "grad_norm": 0.7424572110176086,
      "learning_rate": 9.719945797951675e-05,
      "loss": 2.8759,
      "step": 1250
    },
    {
      "epoch": 0.32900980702309396,
      "grad_norm": 1.0142822265625,
      "learning_rate": 9.692895438683627e-05,
      "loss": 2.8879,
      "step": 1300
    },
    {
      "epoch": 0.34166403037013604,
      "grad_norm": 0.9829534888267517,
      "learning_rate": 9.664639435156414e-05,
      "loss": 2.9039,
      "step": 1350
    },
    {
      "epoch": 0.3543182537171781,
      "grad_norm": 0.862928569316864,
      "learning_rate": 9.635185046574695e-05,
      "loss": 2.906,
      "step": 1400
    },
    {
      "epoch": 0.3669724770642202,
      "grad_norm": 0.684027910232544,
      "learning_rate": 9.604539840018321e-05,
      "loss": 2.8696,
      "step": 1450
    },
    {
      "epoch": 0.3796267004112623,
      "grad_norm": 0.7741448879241943,
      "learning_rate": 9.572711688498276e-05,
      "loss": 2.8733,
      "step": 1500
    },
    {
      "epoch": 0.3922809237583043,
      "grad_norm": 0.8009865283966064,
      "learning_rate": 9.53970876893405e-05,
      "loss": 2.8939,
      "step": 1550
    },
    {
      "epoch": 0.4049351471053464,
      "grad_norm": 0.7556995153427124,
      "learning_rate": 9.505539560052905e-05,
      "loss": 2.909,
      "step": 1600
    },
    {
      "epoch": 0.4175893704523885,
      "grad_norm": 0.7408649921417236,
      "learning_rate": 9.470212840211632e-05,
      "loss": 2.9047,
      "step": 1650
    },
    {
      "epoch": 0.43024359379943056,
      "grad_norm": 0.8015015721321106,
      "learning_rate": 9.433737685141305e-05,
      "loss": 2.9206,
      "step": 1700
    },
    {
      "epoch": 0.44289781714647264,
      "grad_norm": 0.832034170627594,
      "learning_rate": 9.396123465615658e-05,
      "loss": 2.91,
      "step": 1750
    },
    {
      "epoch": 0.4555520404935147,
      "grad_norm": 0.6817078590393066,
      "learning_rate": 9.357379845043654e-05,
      "loss": 2.893,
      "step": 1800
    },
    {
      "epoch": 0.4682062638405568,
      "grad_norm": 0.6884402632713318,
      "learning_rate": 9.317516776986866e-05,
      "loss": 2.8836,
      "step": 1850
    },
    {
      "epoch": 0.4808604871875989,
      "grad_norm": 0.6444289684295654,
      "learning_rate": 9.276544502602337e-05,
      "loss": 2.8569,
      "step": 1900
    },
    {
      "epoch": 0.4935147105346409,
      "grad_norm": 0.8229833841323853,
      "learning_rate": 9.234473548011527e-05,
      "loss": 2.9008,
      "step": 1950
    },
    {
      "epoch": 0.506168933881683,
      "grad_norm": 0.8413760662078857,
      "learning_rate": 9.191314721596072e-05,
      "loss": 2.8887,
      "step": 2000
    },
    {
      "epoch": 0.5188231572287251,
      "grad_norm": 0.7692490220069885,
      "learning_rate": 9.147079111221018e-05,
      "loss": 2.877,
      "step": 2050
    },
    {
      "epoch": 0.5314773805757672,
      "grad_norm": 0.5834383368492126,
      "learning_rate": 9.101778081386257e-05,
      "loss": 2.8686,
      "step": 2100
    },
    {
      "epoch": 0.5441316039228092,
      "grad_norm": 0.8569998145103455,
      "learning_rate": 9.055423270306889e-05,
      "loss": 2.901,
      "step": 2150
    },
    {
      "epoch": 0.5567858272698513,
      "grad_norm": 0.7364405393600464,
      "learning_rate": 9.008026586923264e-05,
      "loss": 2.8762,
      "step": 2200
    },
    {
      "epoch": 0.5694400506168934,
      "grad_norm": 0.7533401250839233,
      "learning_rate": 8.959600207841483e-05,
      "loss": 2.8682,
      "step": 2250
    },
    {
      "epoch": 0.5820942739639354,
      "grad_norm": 0.7989473342895508,
      "learning_rate": 8.910156574205121e-05,
      "loss": 2.8661,
      "step": 2300
    },
    {
      "epoch": 0.5947484973109776,
      "grad_norm": 0.6763147115707397,
      "learning_rate": 8.859708388498996e-05,
      "loss": 2.8468,
      "step": 2350
    },
    {
      "epoch": 0.6074027206580196,
      "grad_norm": 0.7763379216194153,
      "learning_rate": 8.80826861128578e-05,
      "loss": 2.8309,
      "step": 2400
    },
    {
      "epoch": 0.6200569440050617,
      "grad_norm": 0.8251469731330872,
      "learning_rate": 8.755850457876347e-05,
      "loss": 2.8927,
      "step": 2450
    },
    {
      "epoch": 0.6327111673521038,
      "grad_norm": 0.6280380487442017,
      "learning_rate": 8.702467394934624e-05,
      "loss": 2.8454,
      "step": 2500
    },
    {
      "epoch": 0.6453653906991459,
      "grad_norm": 0.6702473759651184,
      "learning_rate": 8.648133137017906e-05,
      "loss": 2.879,
      "step": 2550
    },
    {
      "epoch": 0.6580196140461879,
      "grad_norm": 0.7802298665046692,
      "learning_rate": 8.592861643053469e-05,
      "loss": 2.8943,
      "step": 2600
    },
    {
      "epoch": 0.6706738373932299,
      "grad_norm": 0.6800094842910767,
      "learning_rate": 8.536667112752399e-05,
      "loss": 2.8603,
      "step": 2650
    },
    {
      "epoch": 0.6833280607402721,
      "grad_norm": 0.7358947396278381,
      "learning_rate": 8.479563982961571e-05,
      "loss": 2.8854,
      "step": 2700
    },
    {
      "epoch": 0.6959822840873141,
      "grad_norm": 0.5677722096443176,
      "learning_rate": 8.421566923954707e-05,
      "loss": 2.8628,
      "step": 2750
    },
    {
      "epoch": 0.7086365074343562,
      "grad_norm": 0.7850372195243835,
      "learning_rate": 8.362690835663445e-05,
      "loss": 2.9034,
      "step": 2800
    },
    {
      "epoch": 0.7212907307813983,
      "grad_norm": 0.6631876230239868,
      "learning_rate": 8.302950843849436e-05,
      "loss": 2.855,
      "step": 2850
    },
    {
      "epoch": 0.7339449541284404,
      "grad_norm": 0.7861546277999878,
      "learning_rate": 8.242362296218397e-05,
      "loss": 2.8788,
      "step": 2900
    },
    {
      "epoch": 0.7465991774754824,
      "grad_norm": 0.6736059784889221,
      "learning_rate": 8.180940758477175e-05,
      "loss": 2.8508,
      "step": 2950
    },
    {
      "epoch": 0.7592534008225246,
      "grad_norm": 0.6897044777870178,
      "learning_rate": 8.118702010334758e-05,
      "loss": 2.8604,
      "step": 3000
    },
    {
      "epoch": 0.7719076241695666,
      "grad_norm": 0.7634857296943665,
      "learning_rate": 8.05566204144836e-05,
      "loss": 2.8729,
      "step": 3050
    },
    {
      "epoch": 0.7845618475166086,
      "grad_norm": 0.795070469379425,
      "learning_rate": 7.991837047315532e-05,
      "loss": 2.8845,
      "step": 3100
    },
    {
      "epoch": 0.7972160708636508,
      "grad_norm": 0.7932234406471252,
      "learning_rate": 7.927243425113407e-05,
      "loss": 2.873,
      "step": 3150
    },
    {
      "epoch": 0.8098702942106928,
      "grad_norm": 0.553981602191925,
      "learning_rate": 7.861897769486121e-05,
      "loss": 2.829,
      "step": 3200
    },
    {
      "epoch": 0.8225245175577349,
      "grad_norm": 0.832590639591217,
      "learning_rate": 7.795816868281521e-05,
      "loss": 2.8406,
      "step": 3250
    },
    {
      "epoch": 0.835178740904777,
      "grad_norm": 0.8086447715759277,
      "learning_rate": 7.729017698238212e-05,
      "loss": 2.8673,
      "step": 3300
    },
    {
      "epoch": 0.8478329642518191,
      "grad_norm": 0.7937609553337097,
      "learning_rate": 7.661517420624101e-05,
      "loss": 2.8487,
      "step": 3350
    },
    {
      "epoch": 0.8604871875988611,
      "grad_norm": 0.6960878968238831,
      "learning_rate": 7.59333337682752e-05,
      "loss": 2.8637,
      "step": 3400
    },
    {
      "epoch": 0.8731414109459031,
      "grad_norm": 0.893937349319458,
      "learning_rate": 7.524483083902076e-05,
      "loss": 2.8589,
      "step": 3450
    },
    {
      "epoch": 0.8857956342929453,
      "grad_norm": 0.6948588490486145,
      "learning_rate": 7.454984230066371e-05,
      "loss": 2.8523,
      "step": 3500
    },
    {
      "epoch": 0.8984498576399873,
      "grad_norm": 0.6877617239952087,
      "learning_rate": 7.384854670159757e-05,
      "loss": 2.8776,
      "step": 3550
    },
    {
      "epoch": 0.9111040809870294,
      "grad_norm": 0.8207176923751831,
      "learning_rate": 7.31411242105527e-05,
      "loss": 2.8849,
      "step": 3600
    },
    {
      "epoch": 0.9237583043340715,
      "grad_norm": 0.710628092288971,
      "learning_rate": 7.242775657030954e-05,
      "loss": 2.8618,
      "step": 3650
    },
    {
      "epoch": 0.9364125276811136,
      "grad_norm": 0.48997363448143005,
      "learning_rate": 7.17086270510072e-05,
      "loss": 2.8537,
      "step": 3700
    },
    {
      "epoch": 0.9490667510281556,
      "grad_norm": 0.6646195650100708,
      "learning_rate": 7.098392040306001e-05,
      "loss": 2.8508,
      "step": 3750
    },
    {
      "epoch": 0.9617209743751978,
      "grad_norm": 0.7768404483795166,
      "learning_rate": 7.025382280969346e-05,
      "loss": 2.803,
      "step": 3800
    },
    {
      "epoch": 0.9743751977222398,
      "grad_norm": 0.7650391459465027,
      "learning_rate": 6.95185218391122e-05,
      "loss": 2.8555,
      "step": 3850
    },
    {
      "epoch": 0.9870294210692818,
      "grad_norm": 0.8182047605514526,
      "learning_rate": 6.877820639631222e-05,
      "loss": 2.8246,
      "step": 3900
    },
    {
      "epoch": 0.999683644416324,
      "grad_norm": 0.5656609535217285,
      "learning_rate": 6.80330666745495e-05,
      "loss": 2.8334,
      "step": 3950
    },
    {
      "epoch": 1.0121480544131605,
      "grad_norm": 0.8596081137657166,
      "learning_rate": 6.728329410647782e-05,
      "loss": 2.7281,
      "step": 4000
    },
    {
      "epoch": 1.0248022777602024,
      "grad_norm": 0.8331876993179321,
      "learning_rate": 6.652908131496817e-05,
      "loss": 2.7438,
      "step": 4050
    },
    {
      "epoch": 1.0374565011072445,
      "grad_norm": 0.9927542209625244,
      "learning_rate": 6.577062206362215e-05,
      "loss": 2.7478,
      "step": 4100
    },
    {
      "epoch": 1.0501107244542867,
      "grad_norm": 0.7503067851066589,
      "learning_rate": 6.500811120699256e-05,
      "loss": 2.776,
      "step": 4150
    },
    {
      "epoch": 1.0627649478013288,
      "grad_norm": 0.8594722151756287,
      "learning_rate": 6.424174464052372e-05,
      "loss": 2.776,
      "step": 4200
    },
    {
      "epoch": 1.0754191711483707,
      "grad_norm": 0.7482352256774902,
      "learning_rate": 6.34717192502241e-05,
      "loss": 2.7383,
      "step": 4250
    },
    {
      "epoch": 1.0880733944954128,
      "grad_norm": 0.9169026613235474,
      "learning_rate": 6.26982328620848e-05,
      "loss": 2.7739,
      "step": 4300
    },
    {
      "epoch": 1.100727617842455,
      "grad_norm": 0.7162443399429321,
      "learning_rate": 6.192148419125635e-05,
      "loss": 2.7371,
      "step": 4350
    },
    {
      "epoch": 1.113381841189497,
      "grad_norm": 0.707607090473175,
      "learning_rate": 6.114167279099706e-05,
      "loss": 2.7511,
      "step": 4400
    },
    {
      "epoch": 1.126036064536539,
      "grad_norm": 0.7723420262336731,
      "learning_rate": 6.0358999001406156e-05,
      "loss": 2.7575,
      "step": 4450
    },
    {
      "epoch": 1.1386902878835812,
      "grad_norm": 0.7098573446273804,
      "learning_rate": 5.957366389795478e-05,
      "loss": 2.749,
      "step": 4500
    },
    {
      "epoch": 1.1513445112306233,
      "grad_norm": 0.9452178478240967,
      "learning_rate": 5.878586923982793e-05,
      "loss": 2.7611,
      "step": 4550
    },
    {
      "epoch": 1.1639987345776652,
      "grad_norm": 0.7143250107765198,
      "learning_rate": 5.799581741809086e-05,
      "loss": 2.7552,
      "step": 4600
    },
    {
      "epoch": 1.1766529579247074,
      "grad_norm": 0.7778602838516235,
      "learning_rate": 5.7203711403693097e-05,
      "loss": 2.7804,
      "step": 4650
    },
    {
      "epoch": 1.1893071812717495,
      "grad_norm": 0.999950110912323,
      "learning_rate": 5.640975469532358e-05,
      "loss": 2.7526,
      "step": 4700
    },
    {
      "epoch": 1.2019614046187916,
      "grad_norm": 0.8183728456497192,
      "learning_rate": 5.561415126713001e-05,
      "loss": 2.7321,
      "step": 4750
    },
    {
      "epoch": 1.2146156279658336,
      "grad_norm": 0.7287235856056213,
      "learning_rate": 5.481710551631626e-05,
      "loss": 2.7443,
      "step": 4800
    },
    {
      "epoch": 1.2272698513128757,
      "grad_norm": 0.8611195683479309,
      "learning_rate": 5.4018822210631024e-05,
      "loss": 2.7363,
      "step": 4850
    },
    {
      "epoch": 1.2399240746599178,
      "grad_norm": 1.0261605978012085,
      "learning_rate": 5.321950643576123e-05,
      "loss": 2.7474,
      "step": 4900
    },
    {
      "epoch": 1.2525782980069597,
      "grad_norm": 0.9046388268470764,
      "learning_rate": 5.241936354264377e-05,
      "loss": 2.7566,
      "step": 4950
    },
    {
      "epoch": 1.2652325213540019,
      "grad_norm": 0.7704071402549744,
      "learning_rate": 5.161859909470924e-05,
      "loss": 2.7425,
      "step": 5000
    }
  ],
  "logging_steps": 50,
  "max_steps": 10000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.9169316582925517e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
