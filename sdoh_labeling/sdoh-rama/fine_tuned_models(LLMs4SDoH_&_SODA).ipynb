{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SDoH Sample Text with [LLMs4SDoH](https://github.com/BIDS-Xu-Lab/LLMs4SDoH)"
      ],
      "metadata": {
        "id": "ROXjTs_xqpcZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SrVwif4HqtN",
        "outputId": "557be1f8-5947-4518-a53d-652478b2a72e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLMs4SDoH'...\n",
            "remote: Enumerating objects: 27, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 27 (delta 10), reused 20 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (27/27), 10.46 KiB | 10.46 MiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n",
            "/content/LLMs4SDoH\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/BIDS-Xu-Lab/LLMs4SDoH.git\n",
        "%cd LLMs4SDoH\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DOwcdxIJIoY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76d526bb-6e36-48ac-da5b-d674905dfccf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m117.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM, LlamaTokenizer\n",
        "import numpy as np\n",
        "\n",
        "model_name = \"YBXL/SDoH-llama-L1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device).eval()\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "### Instruction:\n",
        "Given a sentence output all SDoH factors that can be inferred from that sentence from\n",
        "the following list: AdverseChildhood, Alcohol, BirthSex, Drug, EducationLevel,\n",
        "EmploymentStatus, EnvironExposure, FinancialIssues, FoodInsecurity, GenderIdentity,\n",
        "Insurance, Isolation, LivingStatus, LocationBornRaised, MaritalStatus, PhysicalActivity,\n",
        "PhysSexAbuse, Race, SexualOrientation, Smoking, and SocialSupport.\n",
        "If the sentence does not mention any SDoH factor then output - nonSDoH.\n",
        "\"\"\"\n",
        "\n",
        "# SDoH categories for parsing\n",
        "sdoh_categories = [\n",
        "    \"AdverseChildhood\", \"Alcohol\", \"BirthSex\", \"Drug\", \"EducationLevel\",\n",
        "    \"EmploymentStatus\", \"EnvironExposure\", \"FinancialIssues\", \"FoodInsecurity\",\n",
        "    \"GenderIdentity\", \"Insurance\", \"Isolation\", \"LivingStatus\", \"LocationBornRaised\",\n",
        "    \"MaritalStatus\", \"PhysicalActivity\", \"PhysSexAbuse\", \"Race\", \"SexualOrientation\",\n",
        "    \"Smoking\", \"SocialSupport\", \"nonSDoH\"\n",
        "]"
      ],
      "metadata": {
        "id": "ZH_U779TmxZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_sdoh_response(response_text):\n",
        "    \"\"\"Parse the model's text response to extract SDoH categories\"\"\"\n",
        "    response_text = response_text.strip().lower()\n",
        "\n",
        "    found_categories = []\n",
        "    for category in sdoh_categories:\n",
        "        if category.lower() in response_text:\n",
        "            found_categories.append(category)\n",
        "\n",
        "    if \"nonsdoh\" in response_text or not found_categories:\n",
        "        return [\"nonSDoH\"]\n",
        "\n",
        "    # Remove nonSDoH if other categories are found\n",
        "    found_categories = [cat for cat in found_categories if cat.lower() != \"nonsdoh\"]\n",
        "\n",
        "    return found_categories if found_categories else [\"nonSDoH\"]"
      ],
      "metadata": {
        "id": "fCEdOB_WprXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import re\n",
        "\n",
        "texts = df_final['Gemma_Cleaned_Comment'].astype(str).tolist()\n",
        "save_every = 5000\n",
        "batch_size = 8\n",
        "output_preds = []\n",
        "\n",
        "\n",
        "\n",
        "pred_file = open(\"model_predictions_log.txt\", \"w\", encoding=\"utf-8\")\n",
        "pred_file.write('TEXT\\tPREDICTED_SDOH\\tRAW_RESPONSE\\n')\n",
        "\n",
        "\n",
        "# Clear GPU cache before starting\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "p5GZ8Bbqqb8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(0, len(texts), batch_size), desc=\"Classifying batched responses\"):\n",
        "    try:\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "\n",
        "        prompts = []\n",
        "        for text in batch_texts:\n",
        "            prompt = f\"{system_prompt}\\n\\n### Input:\\n{text}\\n\\n### Response:\\n\"\n",
        "            prompts.append(prompt)\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            prompts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=50,\n",
        "                do_sample=False,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                temperature=0.1,\n",
        "                repetition_penalty=1.1\n",
        "            )\n",
        "\n",
        "            # Decode only the new tokens (response part)\n",
        "            input_length = inputs.input_ids.shape[1]\n",
        "            response_tokens = outputs[:, input_length:]\n",
        "            responses = tokenizer.batch_decode(response_tokens, skip_special_tokens=True)\n",
        "\n",
        "        for j, (text, response) in enumerate(zip(batch_texts, responses)):\n",
        "            sdoh_categories_found = parse_sdoh_response(response)\n",
        "\n",
        "            pred_label = \";\".join(sdoh_categories_found)\n",
        "\n",
        "            output_preds.append(pred_label)\n",
        "            pred_file.write(f\"{text.strip()}\\t{pred_label}\\t{response.strip()}\\n\")\n",
        "\n",
        "        del inputs, outputs, response_tokens\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Debug: Print some examples from the first batch\n",
        "        if i == 0:\n",
        "            print(\"\\n=== Sample Responses ===\")\n",
        "            for j in range(min(3, len(responses))):\n",
        "                print(f\"Input: {batch_texts[j][:100]}...\")\n",
        "                print(f\"Raw Response: {responses[j]}\")\n",
        "                print(f\"Parsed SDoH: {output_preds[j]}\")\n",
        "                print(\"-\" * 50)\n",
        "\n",
        "        if (i + batch_size) % save_every < batch_size or (i + batch_size) >= len(texts):\n",
        "            df_partial = df_final.iloc[:len(output_preds)].copy()\n",
        "            df_partial['LLama_SDoH_Labels'] = output_preds\n",
        "            filename = f\"sdoH_partial_output_{len(output_preds)}.csv\"\n",
        "            df_partial.to_csv(filename, index=False)\n",
        "            from google.colab import files\n",
        "            files.download(filename)\n",
        "            print(f\" Saved and downloaded: {filename}\")\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        if \"out of memory\" in str(e).lower():\n",
        "            print(f\" Out of memory at batch {i}. Try reducing batch_size to 4 or 2.\")\n",
        "            torch.cuda.empty_cache()\n",
        "            break\n",
        "        else:\n",
        "            print(f\" Error at batch {i}: {e}\")\n",
        "            break\n"
      ],
      "metadata": {
        "id": "Xb3wAcQsolEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if len(output_preds) == len(texts):\n",
        "    df_final[\"LLama_SDoH_Labels\"] = output_preds\n",
        "    final_csv = \"LLama_SDoH_Final.csv\"\n",
        "    df_final.to_csv(final_csv, index=False)\n",
        "    files.download(final_csv)\n",
        "    print(\"Final inference complete!\")\n",
        "else:\n",
        "    print(f\" Partial results: {len(output_preds)}/{len(texts)} processed\")\n",
        "\n",
        "pred_file.close()\n",
        "files.download(\"model_predictions_log.txt\")"
      ],
      "metadata": {
        "id": "bQrGIyu6p52h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHWH4qPRGkgA"
      },
      "source": [
        "# SDoH Sample Text with [SODA](https://github.com/uf-hobi-informatics-lab/SODA_Docker)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf SODA_Docker\n",
        "!git clone https://github.com/uf-hobi-informatics-lab/SODA_Docker.git\n",
        "%cd SODA_Docker\n",
        "!git checkout SDoH_pipeline\n",
        "!git submodule init\n",
        "!git submodule update\n"
      ],
      "metadata": {
        "id": "KdPTySmZTWk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRRe8yUaGtmL"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/uf-hobi-informatics-lab/SODA_Docker.git\n",
        "%cd SODA_Docker\n",
        "!git checkout SDoH_pipeline\n",
        "!git submodule init\n",
        "!git submodule update\n",
        "\n",
        "!pip install seqeval scikit-learn torch pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip uninstall -y transformers tokenizers\n",
        "\n",
        "!pip install transformers tokenizers"
      ],
      "metadata": {
        "id": "UtZmS3WHT2_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification as load_model, AutoTokenizer\n",
        "print(\"transformers is installed and supports ALBERT.\")\n"
      ],
      "metadata": {
        "id": "59S_-Q_PUFoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Wf1TdYc_84Q"
      },
      "outputs": [],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p pretrained_models/SDOH_bert_final"
      ],
      "metadata": {
        "id": "0toIoeDJLqbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOqKkYaFHiWJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "df_final['Gemma_Cleaned_Comment'].to_csv(\"soda_input.csv\", index=False)\n",
        "\n",
        "output_folder = \"encoded_text\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "df_subset = df_final[df_final['Gemma_Cleaned_Comment'].notna()]\n",
        "\n",
        "for idx, row in df_subset.iterrows():\n",
        "    filename = f\"comment_{idx+1}.txt\"\n",
        "    text = str(row['Gemma_Cleaned_Comment'])\n",
        "    with open(os.path.join(output_folder, filename), 'w', encoding='utf-8') as f:\n",
        "        f.write(text)\n",
        "\n",
        "print(f\"Saved {len(df_subset)} comments to encoded_text/ and soda_input.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config.yml\n",
        "gpu_node: 0\n",
        "root_dir: /content/SODA_Docker\n",
        "raw_data_dir: \"\"\n",
        "generate_bio: False\n",
        "encoded_text: True\n",
        "ner_model:\n",
        "  type: bert\n",
        "  path: bert-base-cased\n"
      ],
      "metadata": {
        "id": "3qyp-pKuLy5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls"
      ],
      "metadata": {
        "id": "VVUGVL-iMYli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x run.sh\n"
      ],
      "metadata": {
        "id": "xqXP3bWlL1nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./run.sh -c config.yml -n 0 > soda_log.txt 2>&1"
      ],
      "metadata": {
        "id": "NcG0coC6L3EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls soda_log.txt"
      ],
      "metadata": {
        "id": "OA5uHQG8Qme2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find . -type f -name \"*.csv\""
      ],
      "metadata": {
        "id": "HlIrJW11PVeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tail -n 50 soda_log.txt\n"
      ],
      "metadata": {
        "id": "gjivAOTxQade"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    results = pd.read_csv(\"results/sdoh_output.csv\")\n",
        "    results.head()\n",
        "except FileNotFoundError:\n",
        "    print(\"Output not found. Check soda_log.txt for errors\")\n"
      ],
      "metadata": {
        "id": "pp1p5iZIMRnx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}